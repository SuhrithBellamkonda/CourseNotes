\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}

\title{\textbf{Math 54}}
\author{Suhrith Bellamkonda}
\date{December 2022}

\begin{document}

\maketitle{}
\begin{center}
    University of California, Berkeley
\end{center}
\begin{center}
    Professor Lin Lin, Fall 2022
\end{center}

\section{Linear Equations}

A \textbf{linear equation} in the variables $x_1,\cdots,x_n$ is an equation that can be written in the form

\begin{equation}
a_1x_1+\cdots+a_nx_n = b 
\end{equation}
\hfill \newline
A \textbf{linear system} is a collection of linear equations. A \textbf{solution} of a linear system is a list $(s_1,\cdots,s_n)$ of numbers making each equation a true statement when substituted for $(x_1,\cdots,x_n)$, respectively. The set of all possible solutions is called the \textbf{solution set} of the linear system. Two linear systems are equivalent if they have the same solution set.

\hfill \newline
A linear system can have 0, 1, or $\infty$ solutions. If a system has no solutions, it is \textbf{inconsistent}. Else, it is \textbf{consistent}.

\hfill \newline The essential information of a linear system can be recorded compactly in a rectangular array called a \textbf{matrix}. For a 2-dimensional system of linear equations given by $ax+by=c$ and $dx+ey=f$, the \textbf{coefficient matrix} is written as 
$\begin{Bmatrix}
a & b\\
d & e
\end{Bmatrix}$
and the \textbf{augmented matrix} is written as
$\begin{Bmatrix}
a & b & c\\
d & e & f
\end{Bmatrix}.$

\hfill \newline
A rectangular matrix is in \textbf{row echelon form} if it has the following properties: (1) All nonzero rows are above any rows of all zeros. (2) Each leading entry of a row is in a column to the right of the leading entry of the row above it. (3) All entries in a column below a leading entry are zeros. If a matrix meets these additional criteria, then it is in \textbf{reduced row echelon form}: (4) The leading entry (leftmost nonzero entry) in each nonzero row is 1. (5) Each leading 1 is the only nonzero entry in its column. 

\hfill \newline
To achieve an echelon form, use the following \textbf{elementary row operations} on an augmented matrix. (1) Replace one row by the sum of itself and the multiple of another row. (2) Interchange two rows. (3) Multiply all entries in a row by a nonzero constant. Once the reduced row echelon form is achieved, the solution to the system is indicated by the rightmost column of the resultant matrix.

\hfill \newline A \textbf{pivot position} in a matrix is one that corresponds to a leading 1 in the reduced echelon form of the matrix. \textbf{Pivot columns} are those that contain pivot variables. Variables corresponding to pivot columns are called \textbf{basic variables}. Others are called \textbf{free variables}.

\hfill \newline \noindent THEOREM. Each matrix is row equivalent to one and only one reduced echelon matrix.

\hfill \newline \noindent THEOREM. A linear system is consistent iff the rightmost column of the augmented matrix is not a pivot column. If a linear system is consistent, then the solution set contains either a unique solution (no free variables) or infinitely many solutions ($>1$ free variable).

\hfill \newline
\textbf{Algebraic properties of vectors in $\Re^n$ - omitted.}
\hfill \newline

\noindent Given vectors $\mathbf{v_1},\dots,\mathbf{v_p}$ in $\Re^n$ and given scalars $c_1,\dots,c_p$, the vector
\begin{equation}
\mathbf{y} = c_1\mathbf{v_1}+\cdots+c_p\mathbf{v_p} 
\end{equation}
is called a \textbf{linear combination} of $\mathbf{v_1},\dots,\mathbf{v_p}$ with weights $c_1,\dots,c_p$.

\hfill \newline If $\mathbf{v_1},\dots,\mathbf{v_p}$ are in $\Re^n$, then the set of all linear combinations of $\mathbf{v_1},\dots,\mathbf{v_p}$ is denoted by Span\{$\mathbf{v_1},\dots,\mathbf{v_p}$\} and is called the \textbf{subset of $\Re^n$ spanned} by \textbf{$\mathbf{v_1},\dots,\mathbf{v_p}$.}

\hfill \newline A fundamental idea in linear algebra is to view a linear combination of vectors as the product of a matrix and a vector. If $A$ is an $m \times n$ matrix, with columns $\textbf{a}_1,\dots,\textbf{a}_n$, and if \textbf{x} $\in Re^n$, then $A\textbf{x}$ is the linear combination of the columns of $A$ using the corresponding entries in x as weights. That is, 

\begin{equation}
    A\textbf{x} = 
    \begin{bmatrix}
    \textbf{a}_1 & \cdots & \textbf{a}_n
    \end{bmatrix}
    \begin{bmatrix}
    x_1 \\
    \vdots \\
    x_n
    \end{bmatrix}
    = x_1\textbf{a}_1 + \cdots + x_n\textbf{a}_n.
\end{equation}





\section{Vector Spaces}
A \textbf{vector space} is a nonempty set $V$ of objects, called \textit{vectors}, on which are defined two operations, called \textit{addition} and \textit{scalar multiplication}, subject to 10 axioms listed below. The axioms must hold for all vectors \textbf{u, v} and \textbf{w} in $V$ and for all scalars $c$ and $d$. 

\begin{enumerate}
    \item $\textbf{u}+\textbf{v} \in V$ 
    \item $\textbf{u}+\textbf{v}$ =  $\textbf{v}+\textbf{u}$
    \item $(\textbf{u}+\textbf{v})+\textbf{w}$ = $\textbf{u}+(\textbf{v}+\textbf{w})$
    \item $\exists$ \textbf{0} $\in$ $V$ s.t. $\textbf{u}+\textbf{0}=\textbf{u}$
    \item For each \textbf{u} $\in V$, $\exists -\textbf{u} \in V$ s.t. $\textbf{u}+(-\textbf{u})=\textbf{0}$ 
    \item $c\textbf{u} \in V$
    \item $c(\textbf{u} +\textbf{v})=c\textbf{u}+c\textbf{v}$
    \item $(c+d)\textbf{u}=c\textbf{u}+d\textbf{u}$
    \item $c(d\textbf{u}) = (cd)\textbf{u}$
    \item $1\textbf{u}=\textbf{u}$
\end{enumerate}

\noindent A \textbf{subspace} of a vector space $V$ is a subset $H$ of $V$ that has three properties: (a) The zero vector of $V$ is in $H$, (b) H is closed under vector addition, (c) H is closed under scalar multiplication.

\hfill \newline THEOREM. If $\mathbf{v_1},\dots,\mathbf{v_p}$ are in a vector space $V$, then Span\{$\mathbf{v_1},\dots,\mathbf{v_p}$\} is a subspace of $V$. 

\hfill \newline The \textbf{null space} of an $m \times n$ matrix $A$, written as Nul $A$, is the set of all solutions of the homogeneous equation $A\textbf{x}=\textbf{0}$ in $\Re^n$. Equivalently, Nul $A$ is the set of all \textbf{x} in $\Re^n$ that are mapped into the zero vector of $\Re^m$ via the linear transformation $\textbf{x} \mapsto A\textbf{x}$.

\hfill \newline THEOREM. The null space of an $m \times n$ matrix is a subspace of $\Re^n$.

\hfill \newline The \textbf{column space} of an $m \times n$ matrix $A$, written as Col $A$, is the set of all linear combinations of the columns of $A$. If $A = [\textbf{a}_1\cdots \textbf{a}_n]$, then Col $A =$ Span\{$\mathbf{a_1},\dots,\mathbf{a_n}$\}.

\hfill \newline THEOREM. The column space of an $m \times n$ matrix is a subspace of $\Re^m$.

\hfill \newline A \textbf{linear transformation} $T$ from a vector space $V$ to a vector space $W$ is a rule that assigns to each vector \textbf{x} in $V$ a unique vector $T(\textbf{x})$ in $W$, s.t. (i) $T(\textbf{u}+\textbf{v})=T(\textbf{u})+T(\textbf{v})$ $\forall$ \textbf{u}, \textbf{v} $\in$ $V$, and (ii) $T(c\textbf{u})=cT(\textbf{u})$ $\forall$ \textbf{u} $\in$ $V$, scalars $c$.

\hfill \newline The \textbf{kernel} of $T$ is the set of all \textbf{u} in $V$ s.t. $T(\textbf{u})=\textbf{0}$. It is a subspace of $V$. The \textbf{range} of $T$ is the set of all vectors in $W$ of the form $T(\textbf{x})$ for some \textbf{x} in $W$. It is a subspace of $W$.


%\hfill \newline Let $H$ be a subspace of a vector space $V$/ A set of vectors $B$ in $V$ is a \textbf{basis} for $H$ if (i) $B$ is a linearly independent set, and (ii) the subspace spanned by $B$ coincides with $H$, equivalently $H =$ Span $B$.




\section{Eigenvalues and Eigenvectors}

$\lambda$ is an \textbf{eigenvalue} of an $n \times n$ matrix $A$ and  \(\textbf{x}\) is the corresponding \textbf{eigenvector} if there is a nontrivial solution to 
\begin{equation}
    A \textbf{x} = \lambda \textbf{x}
\end{equation}
Equivalently, 
\begin{equation}
    (A-\lambda I)\mathbf{x} = \mathbf{0}
\end{equation}
The set of all solutions to the above equation is the null space of matrix $A-\lambda I$. So this set is a \textit{subspace} of $\Re^n$ and is called the \textbf{eigenspace} of $A$ corresponding to $\lambda$.

\hfill \newline THEOREM. The eigenvalues of a triangular matrix are the entries on its main diagonal.

\hfill \newline THEOREM. If $\mathbf{v_1},\dots,\mathbf{v_r}$ are eigenvectors that correspond to distinct eigenvalues $\mathbf{\lambda_1},\dots,\mathbf{\lambda_r}$ of an $n \times n$ matrix A, then the set {$\mathbf{v_1},\dots,\mathbf{v_r}$} is linearly independent.

\hfill \newline A scalar $\lambda$ is an eigenvalue of an $n \times n$ matrix $A$ iff $\lambda$ satisfies the \textbf{characteristic equation}
\begin{equation}
    det(A-\lambda I) = \mathbf{0}
\end{equation}

\hfill \newline If $A$ and $B$ are $n \times n$ matrices, then $A$ \textbf{is similar to} $B$ if there is an invertible matrix $P$ such that $P^{-1}AP=B$, or, equivalently, $A = PBP^{-1}$. Writing $Q$ for $P^{-1}$, we have $Q^{-1}BQ=A$. So $B$ is also similar to $A$, and we simply say that $A$ and $B$ \textbf{are similar}. Changing $A$ into $P^{-1}AP$ is called a \textbf{similarity transformation}.

\hfill \newline THEOREM. If $n \times n$ matrices $A$ and $B$ are similar, then they have the same characteristic polynomial and hence the same eigenvalues (with the same multiplicities).

\hfill \newline A square matrix $A$ is \textbf{diagonalizable} if $A$ is similar to a diagonal matrix, that is, if $A=PDP^{-1}$ for some invertible matrix $P$ and some diagonal matrix $D$.

\hfill \newline THEOREM. An $n \times n$ matrix $A$ is diagonalizable iff $A$ has $n$ linearly independent eigenvectors. In fact,  the columns of $P$ are $n$ linearly independent eigenvectors of $A$. In this case, the diagonal entries of $D$ are eigenvalues of $A$ that correspond, respectively, to the eigenvectors in $P$.

\hfill \newline Alternatively, $A$ is diagonalizable iff there are enough eigenvectors to form a basis (called an \textbf{eigenvector basis} of $\Re^n$.

\hfill \newline THEOREM. An $n \times n$ matrix with $n$ distinct eigenvalues is diagonalizable.

\hfill \newline When an $n \times n$ matrix $A$ has $n$ distinct eigenvalues, $P$ is automatically invertible. When $A$ is diagonalizable but has fewer than $n$ distinct eigenvalues, we can still build $P$ in a way that makes $P$ automatically invertible. See the below theorem.

\hfill \newline THEOREM. Let $A$ be an $n \times n$ matrix whose distinct eigenvalues are $\lambda_1,\dots,\lambda_p$. (1) For $1 \leq k \leq p$, the dimension of the eigenspace for $\lambda_k$ is less than or equal to the multiplicity of the eigenvalue $\lambda_k$. (2) The matrix $A$ is diagonalizable iff the sum of the dimensions of the eigenspaces equals $n$, and this happens iff (i) the characteristic polynomial factors completely into linear factors and (ii) the dimension of the eigenspace for each $\lambda_k$ equals the multiplicity of $\lambda_k$. (3) If A is diagonalizable and $B_k$ is a basis for the eigenspace corresponding to $\lambda_k$ for each $k$, then the total collection of vectors in the sets $B_1,\dots,B_p$ forms an eigenvector basis for $\Re^n$.

\section{Orthogonality and Least Squares}


\end{document}
